[PIPELINE]
# Number of rows processed per chunk
chunk_size = 50000

# Maximum rows to process (-1 means process all rows)
max_rows = -1

# Enable idempotent processing via checkpointing
enable_checkpoint = true

# Path to checkpoint file (JSON)
checkpoint_file = checkpoint.json

[INPUT]
# Input source type: file | directory
input_type = file

# Path to input file or directory
input_path = input/sales_data_old.csv

# Used only when input_type = directory
file_pattern = sales_data_part_*.csv

[OUTPUT]
# Directory where processed outputs will be written
output_dir = ./processed
format = csv

[CHECKPOINTS]
bronze_checkpoint = ./checkpoints/bronze.json
silver_checkpoint = ./checkpoints/silver.json

# csv | parquet | orc Output format (currently only for the gold layer)
format = csv

[MEMORY]
# Soft memory cap per chunk (in MB)
max_chunk_mb = 256

# Flush intermediate aggregations after N rows
flush_interval = 50000

[ANOMALY]
# Number of top anomaly records to retain
top_n = 5

# Revenue threshold to flag suspicious transactions
high_revenue_threshold = 1000000