[PIPELINE]
chunk_size = 10000 # Number of rows processed per chunk
max_rows = -1 # Maximum rows to process (-1 means process all rows)
enable_checkpoint = true # Enable idempotent processing via checkpointing
checkpoint_file = checkpoint.json # Path to checkpoint file (JSON)


[INPUT]
input_type = file # Input source type: file | directory
input_path = input/sales_data_old.csv # Path to input file or directory
file_pattern = sales_data_part_*.csv # Used only when input_type = directory


[OUTPUT]
output_dir = ./processed # Directory where processed outputs will be written
format = parquet   # csv | parquet | orc Output format (currently only for the gold layer)


[MEMORY]
max_chunk_mb = 128 # Soft memory cap per chunk (in MB)
flush_interval = 50000# Flush intermediate aggregations after N rows


[ANOMALY]
top_n = 5# Number of top anomaly records to retain
high_revenue_threshold = 1000000 # Revenue threshold to flag suspicious transactions